# Survivor AI - System Architecture & Algorithms

This document provides a comprehensive breakdown of the system architecture, algorithms, and methodologies used in Survivor AI.

## Table of Contents
1. [System Overview](#system-overview)
2. [Machine Learning Pipeline](#machine-learning-pipeline)
3. [Feature Engineering](#feature-engineering)
4. [Prediction Models](#prediction-models)
5. [Optimization Algorithms](#optimization-algorithms)
6. [Data Integration](#data-integration)
7. [Pool Strategy Logic](#pool-strategy-logic)
8. [Monte Carlo Simulation](#monte-carlo-simulation)

---

## System Overview

Survivor AI is a multi-layered system that combines:
- **Data Collection** from multiple sources (The Odds API, SurvivorGrid)
- **Feature Engineering** to extract predictive features
- **Machine Learning Models** for win probability prediction
- **Hungarian Algorithm** for optimal team assignment
- **Pool Strategy Analysis** for contrarian vs. consensus recommendations
- **Monte Carlo Simulation** for variance and risk analysis

### Architecture Diagram
```
┌─────────────────────────────────────────────────────────────┐
│                     Data Collection Layer                    │
├─────────────────────────────────────────────────────────────┤
│  The Odds API  │  SurvivorGrid  │  Schedule Data  │  Cache  │
└────────────┬────────────────────────────────────────────────┘
             │
             ▼
┌─────────────────────────────────────────────────────────────┐
│                  Feature Engineering Layer                   │
├─────────────────────────────────────────────────────────────┤
│  • Pythagorean Expectation (exp=2.37)                       │
│  • Elo Rating System (K=20, Home=65)                        │
│  • Offensive Metrics (PPG, YPP, 3rd Down %)                 │
│  • Defensive Metrics (PA, YPP allowed, Sacks)               │
│  • Recent Form (Last 5 games, Momentum)                     │
│  • Rest Advantage (Days between games)                      │
└────────────┬────────────────────────────────────────────────┘
             │
             ▼
┌─────────────────────────────────────────────────────────────┐
│                 Machine Learning Layer                       │
├─────────────────────────────────────────────────────────────┤
│  Random Forest  │  Neural Network  │  XGBoost  │  Ensemble  │
│  (100 trees)    │  (100-50-25)     │  (100 est) │  (Weighted)│
└────────────┬────────────────────────────────────────────────┘
             │
             ▼
┌─────────────────────────────────────────────────────────────┐
│                   Optimization Layer                         │
├─────────────────────────────────────────────────────────────┤
│  Hungarian Algorithm  │  Pool Calculator  │  Monte Carlo    │
│  (Linear Assignment)  │  (EV Analysis)    │  (Risk Sim)     │
└────────────┬────────────────────────────────────────────────┘
             │
             ▼
┌─────────────────────────────────────────────────────────────┐
│                  Recommendation Engine                       │
├─────────────────────────────────────────────────────────────┤
│  • Top 5 picks with complete season paths                   │
│  • Pool-adjusted scores                                     │
│  • Risk/variance metrics                                    │
│  • Confidence intervals                                     │
└─────────────────────────────────────────────────────────────┘
```

---

## Machine Learning Pipeline

### Research Foundation

Based on "Advancing NFL win prediction: from Pythagorean formulas to machine learning algorithms" (Frontiers in Sports), our ML pipeline implements:

1. **Neural Networks** - Achieved highest accuracy in peer-reviewed research
2. **Random Forest** - Excellent balance of accuracy and interpretability  
3. **Traditional Methods** - Pythagorean expectation as baseline

### Implementation

```python
# Ensemble prediction with weighted averaging
prediction = (
    0.4 * random_forest_prediction +
    0.3 * neural_network_prediction +
    0.3 * xgboost_prediction
)
```

**Why Ensemble?**
- Reduces overfitting
- Captures different patterns
- More robust to data noise
- Better generalization

---

## Feature Engineering

### 1. Pythagorean Expectation

**Formula:**
```
P(win) = PF^2.37 / (PF^2.37 + PA^2.37)
```

Where:
- PF = Points For (average per game)
- PA = Points Against (average per game)
- 2.37 = NFL-optimized exponent (research-backed)

**Why 2.37?**
Research shows this exponent minimizes prediction error for NFL games, accounting for:
- Variance in NFL scoring
- Impact of field position
- Importance of red zone efficiency

### 2. Elo Rating System

**Adapted for NFL:**
```
Expected Win Probability = 1 / (1 + 10^(-elo_diff/400))

New Elo = Old Elo + K * (Actual - Expected)
```

Parameters:
- Starting Elo: 1500
- K-factor: 20 (adjustment rate)
- Home advantage: ~65 Elo points

**Update Process:**
1. Calculate expected result before game
2. Compare to actual result
3. Adjust Elo based on surprise factor
4. Higher K = faster adjustment to recent form

### 3. Offensive Metrics

Extracted features:
```python
{
    'points_per_game': float,      # Scoring efficiency
    'yards_per_play': float,       # Overall efficiency
    'third_down_pct': float,       # Sustaining drives
    'redzone_efficiency': float,   # Converting opportunities
    'turnover_rate': float,        # Ball security
}
```

### 4. Defensive Metrics

```python
{
    'points_allowed': float,       # Defensive effectiveness
    'yards_allowed_per_play': float,
    'sacks_per_game': float,       # Pass rush
    'takeaway_rate': float,        # Creating turnovers
    'pass_defense_rating': float,  # Coverage quality
}
```

### 5. Recent Form Analysis

**Momentum Score:**
```python
momentum = Σ(w_i * result_i) / Σ(w_i)

where w_i = decay^i  # Exponential decay
```

More recent games weighted more heavily using exponential decay.

### 6. Rest Advantage

```python
rest_advantage = tanh((team_rest - opponent_rest) / 7)
```

Normalized to [-1, 1] range, accounting for:
- Thursday night games (short week)
- Post-bye week advantage
- Extra rest for prime time games

---

## Prediction Models

### Random Forest Regressor

**Configuration:**
```python
RandomForestRegressor(
    n_estimators=100,      # 100 decision trees
    max_depth=None,        # Unlimited depth
    min_samples_split=5,   # Prevent overfitting
    min_samples_leaf=2,    # Minimum samples per leaf
    n_jobs=-1              # Parallel processing
)
```

**Advantages:**
- Handles non-linear relationships
- Feature importance analysis
- Robust to outliers
- No feature scaling needed

### Neural Network

**Architecture:**
```
Input Layer (n_features)
    ↓
Hidden Layer 1 (100 neurons, ReLU)
    ↓
Hidden Layer 2 (50 neurons, ReLU)
    ↓
Hidden Layer 3 (25 neurons, ReLU)
    ↓
Output Layer (1 neuron, Linear)
```

**Configuration:**
```python
MLPRegressor(
    hidden_layer_sizes=(100, 50, 25),
    activation='relu',           # Non-linearity
    solver='adam',               # Adaptive learning
    learning_rate_init=0.001,    # Initial LR
    early_stopping=True,         # Prevent overfitting
    alpha=0.0001                 # L2 regularization
)
```

### XGBoost

**Configuration:**
```python
XGBRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=6,
    objective='reg:squarederror'
)
```

**Why XGBoost?**
- Gradient boosting for sequential learning
- Built-in regularization
- Handles missing values
- Fast training and prediction

---

## Optimization Algorithms

### Hungarian Algorithm

**Purpose:** Find optimal team-to-week assignment that maximizes overall win-out probability.

**Problem Formulation:**
```
Maximize: Π(i=1 to n) P(win_i)

Subject to:
- Each team used at most once
- One team picked per week
```

**Solution Approach:**

1. **Convert to minimization:**
   ```
   Maximize Π P_i = Minimize Σ(-log(P_i))
   ```

2. **Create cost matrix:**
   ```
   C[team][week] = -log(P[team wins in week])
   ```

3. **Apply linear sum assignment:**
   ```python
   from scipy.optimize import linear_sum_assignment
   row_ind, col_ind = linear_sum_assignment(cost_matrix)
   ```

4. **Extract optimal path:**
   ```python
   optimal_path = [(teams[i], weeks[j]) for i, j in zip(row_ind, col_ind)]
   ```

**Why This Works:**
- Logarithm converts product to sum
- Linear assignment is polynomial time O(n³)
- Guarantees global optimum
- Handles constraint elegantly

### Pool Size Adjustment

**Expected Value Calculation:**

```python
EV = P(survive) * (pool_size / expected_survivors)

where:
expected_survivors = pool_size * (
    pick_pct * win_prob +           # Those who picked winner
    (1 - pick_pct) * avg_other_rate # Others who survived
)
```

**Strategy by Pool Size:**

| Pool Size | Strategy | Weight (Win Prob : EV) |
|-----------|----------|------------------------|
| < 10      | Safety-first | 80% : 20% |
| 10-50     | Balanced | 60% : 40% |
| 50-200    | Value-seeking | 40% : 60% |
| 200+      | Contrarian | 20% : 80% |

**Rationale:**
- Small pools: Outlast few opponents with safe picks
- Large pools: Need differentiation, seek value
- Very large: Must be contrarian to stand out

---

## Monte Carlo Simulation

### Purpose
Quantify variance and risk in survivor pool paths.

### Process

1. **Simulate N seasons (typically 10,000):**
   ```python
   for sim in range(n_simulations):
       survived = True
       for week in path:
           if random() > win_probability[week]:
               survived = False
               break
       results[sim] = survived
   ```

2. **Calculate statistics:**
   ```python
   mean_win_out = sum(results) / n_simulations
   std_dev = sqrt(mean_win_out * (1 - mean_win_out) / n)
   ci_95 = mean_win_out ± 1.96 * std_dev
   ```

3. **Sensitivity analysis:**
   - Vary probabilities by ±10%
   - Measure impact on outcomes
   - Calculate downside risk

### Metrics Produced

- **Expected Win-Out Probability:** Mean across simulations
- **Standard Deviation:** Measure of variance
- **95% Confidence Interval:** Range of likely outcomes
- **Survival by Week:** Week-by-week survival rates
- **Sharpe Ratio:** Risk-adjusted returns
- **Percentiles:** 10th, 25th, 50th, 75th, 90th

---

## Data Integration

### Multi-Source Blending

**Priority System:**
1. **The Odds API** (Current week + 1-2 weeks)
   - Most accurate for near-term
   - Real-time market odds
   - High refresh rate

2. **SurvivorGrid** (All weeks)
   - Future week projections
   - Consensus pick percentages
   - Historical patterns

3. **ML Models** (Enhancement layer)
   - 70% ML prediction
   - 30% market odds
   - Blended final probability

**Merge Logic:**
```python
if odds_api_available and week <= current_week + 2:
    use_odds_api_probability()
    enhance_with_ml(weight=0.7)
else:
    use_survivorgrid_projection()
    enhance_with_ml(weight=0.7)
```

### Cache Strategy

- **Expiry:** 6 hours (configurable)
- **Invalidation:** On odds API update
- **Storage:** Pickle format for efficiency
- **Metadata:** Timestamp and source tracking

---

## Pool Strategy Logic

### Contrarian Value Formula

```python
contrarian_value = win_probability * (1 - pick_percentage)
```

**Interpretation:**
- High when: Good team, unpopular pick
- Low when: Popular team (even if good)
- Goal: Maximize survival while differentiating

### Composite Score

```python
if pool_size > 100:
    score = 0.4 * win_prob + 0.6 * EV
else:
    score = 0.8 * win_prob + 0.2 * EV
```

**Reasoning:**
- Large pools: EV matters more (need to differentiate)
- Small pools: Win probability paramount (survive longer)

---

## Performance Characteristics

### Time Complexity

| Operation | Complexity | Notes |
|-----------|------------|-------|
| Feature Engineering | O(n) | n = number of games |
| ML Prediction | O(n * m) | m = features |
| Hungarian Algorithm | O(n³) | n = min(teams, weeks) |
| Monte Carlo | O(s * w) | s = simulations, w = weeks |

### Space Complexity

| Component | Space | Notes |
|-----------|-------|-------|
| Feature Matrix | O(n * m) | n games, m features |
| Cost Matrix | O(t * w) | t teams, w weeks |
| Simulation Results | O(s) | s simulations |
| Cache | O(data_size) | Configurable limit |

---

## Extension Points

### Adding New Features

1. Create feature extraction method in `feature_engineering.py`
2. Add to `extract_comprehensive_features()`
3. Retrain models with new features
4. Validate improvement with cross-validation

### Adding New Models

1. Extend `BasePredictor` class
2. Implement `build_model()` method
3. Add to ensemble with appropriate weight
4. Test on historical data

### Custom Optimization

1. Implement custom optimizer in `optimizer/`
2. Follow interface: `get_top_picks(current_week, n_picks)`
3. Return standardized pick format
4. Integrate into main workflow

---

## References

1. "Advancing NFL win prediction: from Pythagorean formulas to machine learning algorithms"
   - Frontiers in Sports and Active Living
   - DOI: 10.3389/fspor.2025.1638446

2. "Real-time NFL Win Prediction with Advanced Machine Learning"
   - StatsUrge Research

3. Hungarian Algorithm (Kuhn-Munkres)
   - Complexity: O(n³)
   - Guarantees global optimum

4. Elo Rating System
   - Adapted for NFL with K=20
   - Home field advantage ~65 points

---

## Conclusion

Survivor AI combines cutting-edge machine learning with classical optimization algorithms to provide the most sophisticated NFL Survivor Pool analysis available. The system is:

- **Research-backed**: Built on peer-reviewed findings
- **Robust**: Multiple models and simulations
- **Adaptive**: Pool-size aware strategies
- **Comprehensive**: End-to-end pipeline
- **Extensible**: Modular architecture

The integration of ML predictions, optimization algorithms, and risk analysis provides users with actionable insights for maximizing their survivor pool success.
